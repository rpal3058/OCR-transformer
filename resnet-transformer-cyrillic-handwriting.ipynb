{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-03-19T15:14:32.421050Z","iopub.status.busy":"2024-03-19T15:14:32.420660Z","iopub.status.idle":"2024-03-19T15:14:34.406418Z","shell.execute_reply":"2024-03-19T15:14:34.405524Z","shell.execute_reply.started":"2024-03-19T15:14:32.420966Z"},"trusted":true},"outputs":[],"source":["import os\n","import math\n","import string\n","import random\n","import time\n","import wandb\n","import torch\n","import torch.nn as nn\n","from torch import optim\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{},"source":["## CONFIG"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T15:14:34.408525Z","iopub.status.busy":"2024-03-19T15:14:34.408147Z","iopub.status.idle":"2024-03-19T15:14:34.471760Z","shell.execute_reply":"2024-03-19T15:14:34.471079Z","shell.execute_reply.started":"2024-03-19T15:14:34.408490Z"},"trusted":true},"outputs":[],"source":["class Hparams():\n","    def __init__(self):\n","\n","        # SETS OF CHARACTERS\n","        self.cyrillic = ['PAD', 'SOS', ' ', '!', '\"', '%', '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[', ']', '«', '»', 'А', 'Б', 'В', 'Г', 'Д', 'Е', 'Ж', 'З', 'И', 'Й', 'К', 'Л', 'М', 'Н', 'О', 'П', 'Р', 'С', 'Т', 'У', 'Ф', 'Х', 'Ц', 'Ч', 'Ш', 'Щ', 'Э', 'Ю', 'Я', 'а', 'б', 'в', 'г', 'д', 'е', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'ё', 'EOS']\n","        # CHARS TO REMOVE\n","        self.del_sym = []\n","\n","        self.lr = 0.01\n","        self.batch_size = 16\n","        self.hidden = 512\n","        self.enc_layers = 2\n","        self.dec_layers = 2\n","        self.nhead = 4\n","        self.dropout = 0.0\n","        \n","        # IMAGE SIZE\n","        self.width = 256\n","        self.height = 64\n","\n","WANDB_LOG = False\n","\n","PATH_TEST_DIR = '../input/cyrillic-handwriting-dataset/test/'\n","PATH_TEST_LABELS = '../input/cyrillic-handwriting-dataset/test.tsv'\n","PATH_TRAIN_DIR = '../input/cyrillic-handwriting-dataset/train/'\n","PATH_TRAIN_LABELS = '../input/cyrillic-handwriting-dataset/train.tsv'\n","        \n","torch.manual_seed(42)\n","torch.cuda.manual_seed(42)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","hp = Hparams()\n","\n","# CREATE MAPS FROM CHARACTERS TO INDICIES AND VISA VERSA\n","char2idx = {char: idx for idx, char in enumerate(hp.cyrillic)}\n","idx2char = {idx: char for idx, char in enumerate(hp.cyrillic)}"]},{"cell_type":"markdown","metadata":{},"source":["## UTILITY FUNCTIONS"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T15:14:34.474221Z","iopub.status.busy":"2024-03-19T15:14:34.473845Z","iopub.status.idle":"2024-03-19T15:14:43.447777Z","shell.execute_reply":"2024-03-19T15:14:43.446773Z","shell.execute_reply.started":"2024-03-19T15:14:34.474183Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting editdistance\n","  Downloading editdistance-0.6.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n","\u001b[K     |████████████████████████████████| 282 kB 6.4 MB/s eta 0:00:01\n","\u001b[?25hInstalling collected packages: editdistance\n","Successfully installed editdistance-0.6.2\n","\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}],"source":["!pip install editdistance\n","import editdistance\n","\n","def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# text to array of indicies\n","def text_to_labels(s, char2idx):\n","    return [char2idx['SOS']] + [char2idx[i] for i in s if i in char2idx.keys()] + [char2idx['EOS']]\n","\n","# convert images and labels into defined data structures\n","def process_data(image_dir, labels_dir, ignore=[]):\n","    \"\"\"\n","    params\n","    ---\n","    image_dir : str\n","      path to directory with images\n","    labels_dir : str\n","      path to tsv file with labels\n","    returns\n","    ---\n","    img2label : dict\n","      keys are names of images and values are correspondent labels\n","    chars : list\n","      all unique chars used in data\n","    all_labels : list\n","    \"\"\"\n","\n","    chars = []\n","    img2label = dict()\n","\n","    raw = open(labels_dir, 'r', encoding='utf-8').read()\n","    temp = raw.split('\\n')\n","    for t in temp:\n","        try:\n","            x = t.split('\\t')\n","            flag = False\n","            for item in ignore:\n","                if item in x[1]:\n","                    flag = True\n","            if flag == False:\n","                img2label[image_dir + x[0]] = x[1]\n","                for char in x[1]:\n","                    if char not in chars:\n","                        chars.append(char)\n","        except:\n","            print('ValueError:', x)\n","            pass\n","\n","    all_labels = sorted(list(set(list(img2label.values()))))\n","    chars.sort()\n","    chars = ['PAD', 'SOS'] + chars + ['EOS']\n","\n","    return img2label, chars, all_labels\n","\n","# MAKE TEXT TO BE THE SAME LENGTH\n","class TextCollate():\n","    def __call__(self, batch):\n","        x_padded = []\n","        max_y_len = max([i[1].size(0) for i in batch])\n","        y_padded = torch.LongTensor(max_y_len, len(batch))\n","        y_padded.zero_()\n","\n","        for i in range(len(batch)):\n","            x_padded.append(batch[i][0].unsqueeze(0))\n","            y = batch[i][1]\n","            y_padded[:y.size(0), i] = y\n","\n","        x_padded = torch.cat(x_padded)\n","        return x_padded, y_padded\n","\n","\n","# TRANSLATE INDICIES TO TEXT\n","def labels_to_text(s, idx2char):\n","    \"\"\"\n","    params\n","    ---\n","    idx2char : dict\n","        keys : int\n","            indicies of characters\n","        values : str\n","            characters\n","    returns\n","    ---\n","    S : str\n","    \"\"\"\n","    S = \"\".join([idx2char[i] for i in s])\n","    if S.find('EOS') == -1:\n","        return S\n","    else:\n","        return S[:S.find('EOS')]\n","\n","\n","# COMPUTE CHARACTER ERROR RATE\n","def char_error_rate(p_seq1, p_seq2):\n","    \"\"\"\n","    params\n","    ---\n","    p_seq1 : str\n","    p_seq2 : str\n","    returns\n","    ---\n","    cer : float\n","    \"\"\"\n","    p_vocab = set(p_seq1 + p_seq2)\n","    p2c = dict(zip(p_vocab, range(len(p_vocab))))\n","    c_seq1 = [chr(p2c[p]) for p in p_seq1]\n","    c_seq2 = [chr(p2c[p]) for p in p_seq2]\n","    return editdistance.eval(''.join(c_seq1),\n","                             ''.join(c_seq2)) / max(len(c_seq1), len(c_seq2))\n","\n","\n","# RESIZE AND NORMALIZE IMAGE\n","def process_image(img):\n","    \"\"\"\n","    params:\n","    ---\n","    img : np.array\n","    returns\n","    ---\n","    img : np.array\n","    \"\"\"\n","    w, h, _ = img.shape\n","    new_w = hp.height\n","    new_h = int(h * (new_w / w))\n","    img = cv2.resize(img, (new_h, new_w))\n","    w, h, _ = img.shape\n","\n","    img = img.astype('float32')\n","    \n","\n","    new_h = hp.width\n","    if h < new_h:\n","        add_zeros = np.full((w, new_h - h, 3), 255)\n","        img = np.concatenate((img, add_zeros), axis=1)\n","\n","    if h > new_h:\n","        img = cv2.resize(img, (new_h, new_w))\n","\n","    return img\n","\n","\n","# GENERATE IMAGES FROM FOLDER\n","def generate_data(img_paths):\n","    \"\"\"\n","    params\n","    ---\n","    names : list of str\n","        paths to images\n","    returns\n","    ---\n","    data_images : list of np.array\n","        images in np.array format\n","    \"\"\"\n","    data_images = []\n","    for path in tqdm(img_paths):\n","        img = cv2.imread(path)\n","        try:\n","            img = process_image(img)\n","            data_images.append(img.astype('uint8'))\n","        except:\n","            print(path)\n","            img = process_image(img)\n","    return data_images\n","\n","def train(model, optimizer, criterion, iterator):\n","    \"\"\"\n","    params\n","    ---\n","    model : nn.Module\n","    optimizer : nn.Object\n","    criterion : nn.Object\n","    iterator : torch.utils.data.DataLoader\n","    returns\n","    ---\n","    epoch_loss / len(iterator) : float\n","        overall loss\n","    \"\"\"\n","    model.train()\n","    epoch_loss = 0\n","    counter = 0\n","    for src, trg in iterator:\n","        counter += 1\n","        if counter % 500 == 0:\n","            print('[', counter, '/', len(iterator), ']')\n","        if torch.cuda.is_available():\n","            src, trg = src.cuda(), trg.cuda()\n","\n","        optimizer.zero_grad()\n","        output = model(src, trg[:-1, :])\n","\n","        loss = criterion(output.view(-1, output.shape[-1]), torch.reshape(trg[1:, :], (-1,)))\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","\n","    return epoch_loss / len(iterator)\n","\n","# GENERAL FUNCTION FROM TRAINING AND VALIDATION\n","def train_all(model,optimizer,criterion,scheduler, train_loader, val_loader,epoch_limit):\n","    train_loss = 0\n","    confuse_dict = dict()\n","    for epoch in range(0, epoch_limit):\n","        print(f'Epoch: {epoch + 1:02}')\n","        print(\"-----------train------------\")\n","        train_loss = train(model, optimizer, criterion, train_loader)\n","        print(\"train loss :\",train_loss)\n","        print(\"\\n-----------valid------------\")\n","        valid_loss = evaluate(model, criterion, val_loader)\n","        print(\"validation loss :\",valid_loss)\n","        print(\"-----------eval------------\")\n","        eval_loss_cer, eval_accuracy = validate(model, val_loader)\n","        scheduler.step(eval_loss_cer)\n","        \n","        if WANDB_LOG and epoch%4 == 0:\n","            wandb.log({'Train loss WER': train_loss, \"Validation loss WER\": valid_loss, 'Validation Word Accuracy': 100 - eval_accuracy,\n","                       'Validation loss CER': eval_loss_cer,'Learning Rate':scheduler._last_lr[0]})\n","\n","\n","\n","def validate(model, dataloader):\n","    \"\"\"\n","    params\n","    ---\n","    model : nn.Module\n","    dataloader :\n","    returns\n","    ---\n","    cer_overall / len(dataloader) * 100 : float\n","    wer_overall / len(dataloader) * 100 : float\n","    \"\"\"\n","    idx2char = dataloader.dataset.idx2char\n","    char2idx = dataloader.dataset.char2idx\n","    model.eval()\n","    show_count = 0\n","    wer_overall = 0\n","    cer_overall = 0\n","    with torch.no_grad():\n","        for (src, trg) in dataloader:\n","            img = np.moveaxis(src[0].numpy(), 0, 2)\n","            if torch.cuda.is_available():\n","              src = src.cuda()\n","\n","            out_indexes = [char2idx['SOS'], ]\n","\n","            for i in range(100):\n","                \n","                trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)\n","                \n","                #output = model.fc_out(model.transformer.decoder(model.pos_decoder(model.decoder(trg_tensor)), memory))\n","                output = model(src,trg_tensor)\n","                out_token = output.argmax(2)[-1].item()\n","                out_indexes.append(out_token)\n","                if out_token == char2idx['EOS']:\n","                    break\n","\n","            out_char = labels_to_text(out_indexes[1:], idx2char)\n","            real_char = labels_to_text(trg[1:, 0].numpy(), idx2char)\n","            wer_overall += int(real_char != out_char)\n","            if out_char:\n","                cer = char_error_rate(real_char, out_char)\n","            else:\n","                cer = 1\n","            \n","            cer_overall += cer\n","            if WANDB_LOG and out_char != real_char:\n","                wandb.log({'Validation Character Accuracy': (1-cer)*100})\n","                wandb.log({\"Validation Examples\": wandb.Image(img, caption=\"Pred: {} Truth: {}\".format(out_char, real_char))})\n","                show_count += 1\n","    \n","    return cer_overall / len(dataloader) * 100, wer_overall / len(dataloader) * 100\n","\n","\n","# SPLIT DATASET INTO TRAIN AND VALID PARTS\n","def train_valid_split(img2label, val_part=0.3):\n","    \"\"\"\n","    params\n","    ---\n","    img2label : dict\n","        keys are paths to images, values are labels (transcripts of crops)\n","    returns\n","    ---\n","    imgs_val : list of str\n","        paths\n","    labels_val : list of str\n","        labels\n","    imgs_train : list of str\n","        paths\n","    labels_train : list of str\n","        labels\n","    \"\"\"\n","\n","    imgs_val, labels_val = [], []\n","    imgs_train, labels_train = [], []\n","\n","    N = int(len(img2label) * val_part)\n","    items = list(img2label.items())\n","    random.shuffle(items)\n","    for i, item in enumerate(items):\n","        if i < N:\n","            imgs_val.append(item[0])\n","            labels_val.append(item[1])\n","        else:\n","            imgs_train.append(item[0])\n","            labels_train.append(item[1])\n","    print('valid part:{}'.format(len(imgs_val)))\n","    print('train part:{}'.format(len(imgs_train)))\n","    return imgs_val, labels_val, imgs_train, labels_train\n","\n","\n","def evaluate(model, criterion, iterator):\n","    \"\"\"\n","    params\n","    ---\n","    model : nn.Module\n","    criterion : nn.Object\n","    iterator : torch.utils.data.DataLoader\n","    returns\n","    ---\n","    epoch_loss / len(iterator) : float\n","        overall loss\n","    \"\"\"\n","    model.eval()\n","    epoch_loss = 0\n","    with torch.no_grad():\n","        for (src, trg) in tqdm(iterator):\n","            src, trg = src.cuda(), trg.cuda()\n","            output = model(src, trg[:-1, :])\n","            loss = criterion(output.view(-1, output.shape[-1]), torch.reshape(trg[1:, :], (-1,)))\n","            epoch_loss += loss.item()\n","    return epoch_loss / len(iterator)\n","\n","# PREPARE DATASET FROM TRAINING\n","# IT CREATES MIXED DATASET: THE FIRST PART COMES FROM REAL DATA AND THE SECOND PART COMES FORM GENERATOR\n","def get_mixed_data(pretrain_image_dir, pretrain_labels_dir, train_image_dir, train_labels_dir, pretrain_part=0.0):\n","    img2label1, chars1, all_words1 = process_data(pretrain_image_dir, pretrain_labels_dir)  # PRETRAIN PART\n","    img2label2, chars2, all_words2 = process_data(train_image_dir, train_labels_dir)  # TRAIN PART\n","    img2label1_list = list(img2label1.items())\n","    N = len(img2label1_list)\n","    for i in range(N):\n","        j = np.random.randint(0, N)\n","        item = img2label1_list[j]\n","        img2label2[item[0]] = item[1]\n","    return img2label2\n","\n","# MAKE PREDICTION\n","def prediction(model, test_dir, char2idx, idx2char):\n","    \"\"\"\n","    params\n","    ---\n","    model : nn.Module\n","    test_dir : str\n","        path to directory with images\n","    char2idx : dict\n","        map from chars to indicies\n","    id2char : dict\n","        map from indicies to chars\n","    returns\n","    ---\n","    preds : dict\n","        key : name of image in directory\n","        value : dict with keys ['p_value', 'predicted_label']\n","    \"\"\"\n","    preds = {}\n","    os.makedirs('/output', exist_ok=True)\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for filename in os.listdir(test_dir):\n","            img = cv2.imread(test_dir + filename)\n","            img = process_image(img).astype('uint8')\n","            img = img / img.max()\n","            img = np.transpose(img, (2, 0, 1))\n","            src = torch.FloatTensor(img).unsqueeze(0).to(device)\n","            p_values = 1\n","            out_indexes = [char2idx['SOS'], ]\n","\n","            for i in range(100):\n","                \n","                trg_tensor = torch.LongTensor(out_indexes).unsqueeze(1).to(device)\n","                \n","                output = model(src,trg_tensor)\n","                out_token = output.argmax(2)[-1].item()\n","                out_indexes.append(out_token)\n","                if out_token == char2idx['EOS']:\n","                    break\n","\n","            pred = labels_to_text(out_indexes[1:], idx2char)\n","            preds[filename] = {'predicted_label': pred, 'p_values': p_values}\n","\n","    return preds"]},{"cell_type":"markdown","metadata":{},"source":["## TRANSFORMER MODEL"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T15:14:43.449771Z","iopub.status.busy":"2024-03-19T15:14:43.449492Z","iopub.status.idle":"2024-03-19T15:14:43.473206Z","shell.execute_reply":"2024-03-19T15:14:43.472251Z","shell.execute_reply.started":"2024-03-19T15:14:43.449740Z"},"trusted":true},"outputs":[],"source":["class TransformerModel(nn.Module):\n","    def __init__(self, bb_name, outtoken, hidden, enc_layers=1, dec_layers=1, nhead=1, dropout=0.1, pretrained=False):\n","        # здесь загружаем сверточную модель, например, resnet50\n","        super(TransformerModel, self).__init__()\n","        self.backbone = models.__getattribute__(bb_name)(pretrained=pretrained)\n","        self.backbone.fc = nn.Conv2d(2048, int(hidden/2), 1)\n","\n","        self.pos_encoder = PositionalEncoding(hidden, dropout)\n","        self.decoder = nn.Embedding(outtoken, hidden)\n","        self.pos_decoder = PositionalEncoding(hidden, dropout)\n","        self.transformer = nn.Transformer(d_model=hidden, nhead=nhead, num_encoder_layers=enc_layers,\n","                                          num_decoder_layers=dec_layers, dim_feedforward=hidden * 4, dropout=dropout,\n","                                          activation='relu')\n","\n","        self.fc_out = nn.Linear(hidden, outtoken)\n","        self.src_mask = None\n","        self.trg_mask = None\n","        self.memory_mask = None\n","        \n","        print('backbone: {}'.format(bb_name))\n","        print('layers: {}'.format(enc_layers))\n","        print('heads: {}'.format(nhead))\n","        print('dropout: {}'.format(dropout))\n","        print(f'{count_parameters(self):,} trainable parameters')\n","\n","    def generate_square_subsequent_mask(self, sz):\n","        mask = torch.triu(torch.ones(sz, sz), 1)\n","        mask = mask.masked_fill(mask == 1, float('-inf'))\n","        return mask\n","\n","    def make_len_mask(self, inp):\n","        return (inp == 0).transpose(0, 1)\n","\n","    def forward(self, src, trg):\n","        '''\n","        params\n","        ---\n","        src : Tensor [64, 3, 64, 256] : [B,C,H,W]\n","            B - batch, C - channel, H - height, W - width\n","        trg : Tensor [13, 64] : [L,B]\n","            L - max length of label\n","        '''\n","        if self.trg_mask is None or self.trg_mask.size(0) != len(trg):\n","            self.trg_mask = self.generate_square_subsequent_mask(len(trg)).to(trg.device) \n","        x = self.backbone.conv1(src)\n","\n","        x = self.backbone.bn1(x)\n","        x = self.backbone.relu(x)\n","        x = self.backbone.maxpool(x)\n","        x = self.backbone.layer1(x)\n","        x = self.backbone.layer2(x)\n","        x = self.backbone.layer3(x)\n","        x = self.backbone.layer4(x) # [64, 2048, 2, 8] : [B,C,H,W]\n","            \n","        x = self.backbone.fc(x) # [64, 256, 2, 8] : [B,C,H,W]\n","        x = x.permute(0, 3, 1, 2) # [64, 8, 256, 2] : [B,W,C,H]\n","        x = x.flatten(2) # [64, 8, 512] : [B,W,CH]\n","        x = x.permute(1, 0, 2) # [8, 64, 512] : [W,B,CH]\n","        \n","        src_pad_mask = self.make_len_mask(x[:, :, 0])\n","        src = self.pos_encoder(x) # [8, 64, 512]\n","        trg_pad_mask = self.make_len_mask(trg)\n","        trg = self.decoder(trg)\n","        trg = self.pos_decoder(trg)\n","\n","        output = self.transformer(src, trg, src_mask=self.src_mask, tgt_mask=self.trg_mask,\n","                                  memory_mask=self.memory_mask,\n","                                  src_key_padding_mask=src_pad_mask, tgt_key_padding_mask=trg_pad_mask,\n","                                  memory_key_padding_mask=src_pad_mask) # [13, 64, 512] : [L,B,CH]\n","        output = self.fc_out(output) # [13, 64, 92] : [L,B,H]\n","\n","        return output\n","\n","class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout=0.1, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.scale = nn.Parameter(torch.ones(1))\n","\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(\n","            0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.scale * self.pe[:x.size(0), :]\n","        return self.dropout(x) "]},{"cell_type":"markdown","metadata":{},"source":["## DATASET CLASS"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T15:14:43.475131Z","iopub.status.busy":"2024-03-19T15:14:43.474771Z","iopub.status.idle":"2024-03-19T15:14:43.488442Z","shell.execute_reply":"2024-03-19T15:14:43.487624Z","shell.execute_reply.started":"2024-03-19T15:14:43.475094Z"},"trusted":true},"outputs":[],"source":["# store list of images' names (in directory) and does some operations with images\n","class TextLoader(torch.utils.data.Dataset):\n","    def __init__(self, images_name, labels, char2idx, idx2char, eval=False):\n","        \"\"\"\n","        params\n","        ---\n","        images_name : list\n","            list of names of images (paths to images)\n","        labels : list\n","            list of labels to correspondent images from images_name list\n","        char2idx : dict\n","        idx2char : dict\n","        \"\"\"\n","        self.images_name = images_name\n","        self.labels = labels\n","        self.char2idx = char2idx\n","        self.idx2char = idx2char\n","        self.eval = eval\n","        self.transform = transforms.Compose([\n","            transforms.ToPILImage(),\n","            p.torch_transform(),  # random distortion and shear\n","            #transforms.Resize((int(hp.height *1.05), int(hp.width *1.05))),\n","            #transforms.RandomCrop((hp.height, hp.width)),\n","            transforms.ColorJitter(contrast=(0.5,1),saturation=(0.5,1)),\n","            transforms.RandomRotation(degrees=(-9, 9), fill=255),\n","            transforms.RandomAffine(10 ,None ,[0.6 ,1] ,3 ,fillcolor=255),\n","            transforms.ToTensor()\n","        ])\n","\n","    def _transform(self, X):\n","        j = np.random.randint(0, 3, 1)[0]\n","        if j == 0:\n","            return self.transform(X)\n","        if j == 1:\n","            return tt(ld(vignet(X)))\n","        if j == 2:\n","            return tt(ld(un(X)))\n","        \n","\n","    def __getitem__(self, index):\n","        img = self.images_name[index]\n","        if not self.eval:\n","            img = self.transform(img)\n","            img = img / img.max()\n","            img = img ** (random.random() * 0.7 + 0.6)\n","        else:\n","            img = np.transpose(img, (2, 0, 1))\n","            img = img / img.max()\n","\n","        label = text_to_labels(self.labels[index], self.char2idx)\n","        return (torch.FloatTensor(img), torch.LongTensor(label))\n","\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"markdown","metadata":{},"source":["## AUGMENTATIONS"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T15:14:43.489757Z","iopub.status.busy":"2024-03-19T15:14:43.489496Z","iopub.status.idle":"2024-03-19T15:14:51.280712Z","shell.execute_reply":"2024-03-19T15:14:51.279734Z","shell.execute_reply.started":"2024-03-19T15:14:43.489732Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting Augmentor\n","  Downloading Augmentor-0.2.12-py2.py3-none-any.whl (38 kB)\n","Requirement already satisfied: Pillow>=5.2.0 in /opt/conda/lib/python3.7/site-packages (from Augmentor) (8.2.0)\n","Requirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from Augmentor) (1.19.5)\n","Requirement already satisfied: tqdm>=4.9.0 in /opt/conda/lib/python3.7/site-packages (from Augmentor) (4.61.1)\n","Installing collected packages: Augmentor\n","Successfully installed Augmentor-0.2.12\n","\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"]}],"source":["!pip install Augmentor\n","import Augmentor\n","from torchvision import transforms\n","import numpy as np\n","import cv2\n","\n","\n","class Vignetting(object):\n","    def __init__(self,\n","                 ratio_min_dist=0.2,\n","                 range_vignette=(0.2, 0.8),\n","                 random_sign=False):\n","        self.ratio_min_dist = ratio_min_dist\n","        self.range_vignette = np.array(range_vignette)\n","        self.random_sign = random_sign\n","\n","    def __call__(self, X, Y=None):\n","        h, w = X.shape[:2]\n","        min_dist = np.array([h, w]) / 2 * np.random.random() * self.ratio_min_dist\n","\n","        # create matrix of distance from the center on the two axis\n","        x, y = np.meshgrid(np.linspace(-w / 2, w / 2, w), np.linspace(-h / 2, h / 2, h))\n","        x, y = np.abs(x), np.abs(y)\n","\n","        # create the vignette mask on the two axis\n","        x = (x - min_dist[0]) / (np.max(x) - min_dist[0])\n","        x = np.clip(x, 0, 1)\n","        y = (y - min_dist[1]) / (np.max(y) - min_dist[1])\n","        y = np.clip(y, 0, 1)\n","\n","        # then get a random intensity of the vignette\n","        vignette = (x + y) / 2 * np.random.uniform(*self.range_vignette)\n","        vignette = np.tile(vignette[..., None], [1, 1, 3])\n","\n","        sign = 2 * (np.random.random() < 0.5) * (self.random_sign) - 1\n","        X = X * (1 + sign * vignette)\n","        return X\n","\n","\n","class LensDistortion(object):\n","    def __init__(self, d_coef=(0.15, 0.05, 0.1, 0.1, 0.05)):\n","        self.d_coef = np.array(d_coef)\n","\n","    def __call__(self, X):\n","        # get the height and the width of the image\n","        h, w = X.shape[:2]\n","\n","        # compute its diagonal\n","        f = (h ** 2 + w ** 2) ** 0.5\n","\n","        # set the image projective to carrtesian dimension\n","        K = np.array([[f, 0, w / 2],\n","                      [0, f, h / 2],\n","                      [0, 0, 1]])\n","\n","        d_coef = self.d_coef * np.random.random(5)  # value\n","        d_coef = d_coef * (2 * (np.random.random(5) < 0.5) - 1)  # sign\n","        # Generate new camera matrix from parameters\n","        M, _ = cv2.getOptimalNewCameraMatrix(K, d_coef, (w, h), 0)\n","\n","        # Generate look-up tables for remapping the camera image\n","        remap = cv2.initUndistortRectifyMap(K, d_coef, None, M, (w, h), 5)\n","\n","        # Remap the original image to a new image\n","        X = cv2.remap(X, *remap, cv2.INTER_LINEAR)\n","        return X\n","\n","\n","class UniformNoise(object):\n","    def __init__(self, low=-50, high=50):\n","        self.low = low\n","        self.high = high\n","\n","    def __call__(self, X):\n","        noise = np.random.uniform(self.low, self.high, X.shape)\n","        X = X + noise\n","        return X\n","    \n","\n","vignet = Vignetting()\n","un = UniformNoise()\n","tt = transforms.ToTensor()\n","p = Augmentor.Pipeline()\n","ld = LensDistortion()\n","p.shear(max_shear_left=2, max_shear_right=2, probability=0.7)\n","p.random_distortion(probability=1.0, grid_width=3, grid_height=3, magnitude=11)\n"]},{"cell_type":"markdown","metadata":{},"source":["## TRAIN\n","\n","I train on real data mixed with synthetic one to avoid forgetting data from the pretraining\n","\n","\n","source.txt and source_valid.txt are just the list of expressions used for generation. Expression are separated by '\\n'.  "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-19T15:14:51.282410Z","iopub.status.busy":"2024-03-19T15:14:51.282071Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 6/65058 [00:00<18:45, 57.80it/s]"]},{"name":"stdout","output_type":"stream","text":["valid part:7228\n","train part:65058\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 21027/65058 [04:11<08:06, 90.46it/s]"]}],"source":["img2label, _, all_words = process_data('../input/cyrillic-handwriting-dataset/train/', '../input/cyrillic-handwriting-dataset/train.tsv') \n","\n","chars = hp.cyrillic\n","\n","X_val, y_val, X_train, y_train = train_valid_split(img2label,val_part=0.1)\n","\n","X_train = generate_data(X_train)\n","X_val = generate_data(X_val)\n","\n","train_dataset = TextLoader(X_train, y_train, char2idx ,idx2char, eval=False)\n","train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True,\n","                                           batch_size=hp.batch_size, pin_memory=True,\n","                                           drop_last=True, collate_fn=TextCollate())\n","\n","val_dataset = TextLoader(X_val, y_val, char2idx,idx2char, eval=True)\n","val_loader = torch.utils.data.DataLoader(val_dataset, shuffle=False,\n","                                         batch_size=1, pin_memory=False,\n","                                         drop_last=False, collate_fn=TextCollate())\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from matplotlib.pyplot import figure\n","\n","figure(figsize=(10, 10), dpi=120)\n","\n","examples = []\n","idx = 0\n","\n","for batch in train_loader:\n","    img = batch[0]\n","    examples.append(img)\n","    idx += 1\n","    if idx == hp.batch_size:\n","        break\n","fig = plt.figure(figsize=(13, 13))\n","rows = int(hp.batch_size / 4) + 2\n","columns = int(hp.batch_size / 8) + 2\n","for j, exp in enumerate(examples):\n","    fig.add_subplot(rows, columns, j + 1)\n","    plt.imshow(exp[0].permute(2, 1, 0).permute(1, 0, 2))"]},{"cell_type":"markdown","metadata":{},"source":["trainig up to the decent metrics requires >10 hours; so, we advice you to train a model through several kaggle sessions or on your own GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = TransformerModel('resnet50', len(hp.cyrillic), hidden=hp.hidden, enc_layers=hp.enc_layers, dec_layers=hp.dec_layers,   \n","                         nhead=hp.nhead, dropout=hp.dropout).to(device)\n","optimizer = optim.SGD(model.parameters(), lr=hp.lr)\n","criterion = nn.CrossEntropyLoss(ignore_index=char2idx['PAD'])\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n","\n","if WANDB_LOG:\n","    wandb.init(project=\"OCR-transformer\", config={\n","        \"learning_rate\":hp.lr,\n","        \"dropout\": hp.dropout,\n","        \"batch_size\": hp.batch_size,\n","        \"architecture\": \"RESNET50 + TRANSFORMER\",\n","        \"dataset\": \"64x512 train 29k\",\n","        \"classes\": \"92\",\n","    })\n","    config = wandb.config\n","    \n","train_all(model, optimizer, criterion, scheduler,train_loader,val_loader, epoch_limit=160)"]},{"cell_type":"markdown","metadata":{},"source":["## TEST"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def test(model, image_dir, label_dir, char2idx, idx2char, case=True, punct=False):\n","    \"\"\"\n","    params\n","    ---\n","    model : pytorch model\n","    image_dir : str\n","        path to the folder with images\n","    label_dir : str\n","        path to the tsv file with labels\n","    char2idx : dict\n","    idx2char : dict\n","    case : bool\n","        if case is False then case of letter is ignored while comparing true and predicted transcript\n","    punct : bool\n","        if punct is False then punctution marks are ignored while comparing true and predicted transcript\n","    returns\n","    ---\n","    character_accuracy : float\n","    string_accuracy : float\n","    \"\"\"\n","    img2label = dict()\n","    raw = open(label_dir, 'r', encoding='utf-8').read()\n","    temp = raw.split('\\n')\n","    for t in temp:\n","        x = t.split('\\t')\n","        img2label[image_dir + x[0]] = x[1]\n","    preds = prediction(model, image_dir, char2idx, idx2char)\n","    N = len(preds)\n","\n","    wer = 0\n","    cer = 0\n","\n","    for item in preds.items():\n","        print(item)\n","        img_name = item[0]\n","        true_trans = img2label[image_dir + img_name]\n","        predicted_trans = item[1]\n","\n","        if 'ё' in true_trans:\n","            true_trans = true_trans.replace('ё', 'е')\n","        if 'ё' in predicted_trans['predicted_label']:\n","            predicted_trans['predicted_label'] = predicted_trans['predicted_label'].replace('ё', 'е')\n","\n","        if not case:\n","            true_trans = true_trans.lower()\n","            predicted_trans['predicted_label'] = predicted_trans['predicted_label'].lower()\n","\n","        if not punct:\n","            true_trans = true_trans.translate(str.maketrans('', '', string.punctuation))\n","            predicted_trans['predicted_label'] = predicted_trans['predicted_label'].translate(str.maketrans('', '', string.punctuation))\n","\n","        if true_trans != predicted_trans['predicted_label']:\n","            print('true:', true_trans)\n","            print('predicted:', predicted_trans)\n","            print('cer:', char_error_rate(predicted_trans['predicted_label'], true_trans))\n","            print('---')\n","            wer += 1\n","            cer += char_error_rate(predicted_trans['predicted_label'], true_trans)\n","\n","    character_accuracy = 1 - cer / N\n","    string_accuracy = 1 - (wer / N)\n","    return character_accuracy, string_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["img2label, _, all_words = process_data('../input/cyrillic-handwriting-dataset/test/', '../input/cyrillic-handwriting-dataset/test.tsv') \n","\n","\n","X_train = generate_data(X_train)\n","X_val = generate_data(X_val)\n","\n","train_dataset = TextLoader(X_train, y_train, char2idx ,idx2char, eval=False)\n","train_loader = torch.utils.data.DataLoader(train_dataset, shuffle=True,\n","                                           batch_size=hp.batch_size, pin_memory=True,\n","                                           drop_last=True, collate_fn=TextCollate())"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = TransformerModel('resnet50', len(hp.cyrillic), hidden=hp.hidden, enc_layers=hp.enc_layers, dec_layers=hp.dec_layers,   \n","                         nhead=hp.nhead, dropout=hp.dropout).to(device)\n","\n","model.load_state_dict(torch.load('../input/weights/ocr_transformer_rn50_64x256_53str_jit.pt')['model'])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["word_accur, char_accur = test(model,PATH_TEST_DIR,PATH_TEST_LABELS,char2idx,idx2char,case=False,punct=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(word_accur, char_accur)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1508751,"sourceId":2492258,"sourceType":"datasetVersion"},{"datasetId":1542177,"sourceId":2543144,"sourceType":"datasetVersion"},{"datasetId":1502872,"sourceId":3977616,"sourceType":"datasetVersion"}],"dockerImageVersionId":30121,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":4}
